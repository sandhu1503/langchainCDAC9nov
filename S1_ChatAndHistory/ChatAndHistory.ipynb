{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae4cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help you with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and the forecast for the rest of the day. \\n\\n- Have a great day!' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '72ac2c3d-9cb2-4e28-b05a-fe21db73852d', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '72ac2c3d-9cb2-4e28-b05a-fe21db73852d', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} id='run-0480acce-0a25-48a9-80fc-bcefcec92959-0' usage_metadata={'input_tokens': 237, 'output_tokens': 46, 'total_tokens': 283}\n",
      "content=\"I'd be happy to help you out!\\n\\nAccording to our latest forecast, today's weather is looking mostly sunny with a high temperature of 72°F (22°C) and a low of 50°F (10°C). It's a beautiful day out there! There's a gentle breeze blowing at about 5 mph, and humidity is at a comfortable 40%. As for precipitation, we're expecting a slight chance of scattered showers later this evening, but nothing too heavy.\\n\\nIf you're planning to head outdoors, make sure to pack a light jacket for the cooler morning and evening hours, and don't forget your sunscreen for that sunny peak hour!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 137, 'prompt_tokens': 51, 'total_tokens': 188, 'completion_time': 0.114166667, 'prompt_time': 0.009298451, 'queue_time': 0.006217918000000001, 'total_time': 0.123465118}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-5345c62f-18a6-4990-87a4-1209348806ee-0' usage_metadata={'input_tokens': 51, 'output_tokens': 137, 'total_tokens': 188}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000cab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure! Here are some websites that can help guide you in your career and study journey:\\n\\n**1. Coursera:**\\n   https://www.coursera.org/\\n\\n   Coursera is a popular online learning platform that offers a wide variety of courses and specializations from top universities and organizations. It covers many different fields, including computer science, data science, business, and more. You can enroll in courses to learn new skills or build upon existing ones.\\n\\n**2. edX:**\\n   https://www.edx.org/\\n\\n   edX is another well-known online learning platform that provides Massive Open Online Courses (MOOCs) from top institutions worldwide. It covers various topics, including programming, business, engineering, humanities, and more. edX also offers professional certificates and micromasters programs to help you gain in-demand skills.\\n\\n**3. Udemy:**\\n   https://www.udemy.com/\\n\\n   Udemy is an online learning platform with a vast library of courses taught by expert instructors. You can find courses on almost any topic, from programming and design to marketing and personal development. Udemy courses are often very practical and can help you learn job-relevant skills.\\n\\n**4. LinkedIn Learning (Lynda):**\\n   https://www.linkedin.com/learning/\\n\\n   LinkedIn Learning, formerly known as Lynda.com, offers a wide range of video courses taught by industry experts. It covers various topics, including business, technology, creative skills, and soft skills. LinkedIn Learning is particularly useful for professional development and can help you build skills that are valuable in the job market.\\n\\n**5. Khan Academy:**\\n   https://www.khanacademy.org/\\n\\n   Khan Academy is a non-profit organization that provides free online courses and lessons for students of all ages. While it started as a resource for math and science, it has since expanded to cover other subjects, including economics, computer programming, and test preparation. Khan Academy is excellent for filling knowledge gaps and building a strong foundation.\\n\\n**6. CareerOneStop:**\\n   https://www.careeronesstop.org/\\n\\n   CareerOneStop is a website sponsored by the U.S. Department of Labor that provides career exploration, training, and job search resources. It offers information on different careers, salary data, job outlook, and required education and training. You can also find advice on resume writing, interviewing, and other job search strategies.\\n\\n**7. Indeed Career Guide:**\\n   https://www.indeed.com/career-guide\\n\\n   Indeed's Career Guide offers a wealth of resources for job seekers. It provides industry-specific career advice, interview tips, resume and cover letter guidance, and more. You can also find salary information, job trends, and other tools to help you navigate your job search effectively.\\n\\n**8. Glassdoor:**\\n   https://www.glassdoor.com/\\n\\n   Glassdoor is a website where you can find job listings and gain insights into specific companies and careers. It provides employee reviews, salary information, and interview questions for various companies. Glassdoor can help you research potential employers and understand the job market better.\\n\\nThese websites offer a wealth of information and resources to guide you in your career and study journey. Remember to explore the ones that align with your interests and goals, and make use of the tools and advice they provide to help you achieve success.\" additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '97aa31cb-78d0-489a-a5e9-7bef9111d37f', 'token_count': {'input_tokens': 226.0, 'output_tokens': 701.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '97aa31cb-78d0-489a-a5e9-7bef9111d37f', 'token_count': {'input_tokens': 226.0, 'output_tokens': 701.0}} id='run-747d1cfc-2552-4e87-9f3d-30ee6c30d2b6-0' usage_metadata={'input_tokens': 226, 'output_tokens': 701, 'total_tokens': 927}\n",
      "content=\"I'd be happy to help you with that. Here are some websites that can guide you on career guidance, study tips, and placement preparation:\\n\\n**Career Guidance:**\\n\\n1. MyNextMove (https://www.mynextmove.org/) - A US-based website that helps you find career matches based on your interests, skills, and work values.\\n2. CareerOneStop (https://www.careeronestop.org/) - A comprehensive career guidance website provided by the US Department of Labor.\\n3. CareerExplorer (https://www.careerexplorer.com/) - A free career test and guidance website that helps you find your ideal career.\\n4. O*NET (https://www.onetonline.org/) - A US-based website that provides career information, job descriptions, and salary ranges.\\n5. CareerBuilder (https://www.careerbuilder.com/) - A job search website that also offers career guidance and advice.\\n\\n**Study Tips:**\\n\\n1. Coursera (https://www.coursera.org/) - A massive open online course platform that offers courses on various subjects, including career development.\\n2. edX (https://www.edx.org/) - A non-profit online learning platform that offers courses and certifications from top universities.\\n3. Khan Academy (https://www.khanacademy.org/) - A free online learning platform that offers courses, exercises, and personalized learning dashboards.\\n4. StudyTips.org (https://www.studytips.org/) - A website that provides study tips, time management strategies, and exam preparation advice.\\n5. BrainPickr (https://www.brainpickr.com/) - A website that offers study tips, productivity advice, and learning resources.\\n\\n**Placement Preparation:**\\n\\n1. Glassdoor (https://www.glassdoor.com/) - A job search website that also offers company reviews, salary information, and interview preparation advice.\\n2. Indeed (https://www.indeed.com/) - A job search website that also offers resume building tools, interview practice, and career advice.\\n3. LinkedIn Learning (https://www.linkedin.com/learning/) - A online learning platform that offers courses and tutorials on various skills, including career development and job search.\\n4. Pluralsight (https://www.pluralsight.com/) - A online learning platform that offers courses and tutorials on various skills, including technology and career development.\\n5. Resume-io (https://www.resume-io.com/) - A website that helps you create a professional resume, cover letter, and LinkedIn profile.\\n\\nRemember, these websites are just a starting point. It's essential to use multiple resources and combine them with your own research and hard work to achieve your career goals. Good luck!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 553, 'prompt_tokens': 40, 'total_tokens': 593, 'completion_time': 0.460833333, 'prompt_time': 0.001594841, 'queue_time': 0.011503638, 'total_time': 0.462428174}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-97f79ecb-0f89-4e6d-bda9-c703cd14c770-0' usage_metadata={'input_tokens': 40, 'output_tokens': 553, 'total_tokens': 593}\n"
     ]
    }
   ],
   "source": [
    "messages = [ \n",
    "    SystemMessage(content='you are career guidence and how to do study for getting placement'),\n",
    "    HumanMessage(content='are you able to give me some websites who guide me?')\n",
    "]\n",
    "\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))         \n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<cohere.client.Client object at 0x72b5bbadf080> async_client=<cohere.client.AsyncClient object at 0x72b5bbadee10> model='command-r' cohere_api_key=SecretStr('**********')\n",
      "['/usr/local/python/3.12.1/lib/python312.zip', '/usr/local/python/3.12.1/lib/python3.12', '/usr/local/python/3.12.1/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/usr/local/python/3.12.1/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A classic lateral thinking puzzle!\\n\\nThe man should choose the third room with the tigers that haven't eaten for six months. Why? Because they're likely to be dead or weak from starvation, making it a less deadly option compared to the other two rooms.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles('hungryLions') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59284572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three friends go to a restaurant and decide to split the bill. The waiter tells them the total bill is ₹300. Each friend contributes ₹100, making a total of ₹300.\n",
      "                After they pay, the waiter realizes he made a mistake — the actual bill was only ₹250. To correct this, he gives ₹50 back to the friends. However, the friends decide to give the waiter ₹20 as a tip and split the remaining ₹30 among themselves, each getting ₹10 back.\n",
      "                Now, each friend has effectively paid ₹90 (their initial ₹100 minus the ₹10 they got back). Together, they have paid ₹270 (₹90 x 3), and the waiter has ₹20. But ₹270 + ₹20 equals ₹290, not ₹300.\n",
      "                Where did the missing ₹10 go?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The missing ₹10 went to the tip. The friends gave ₹20 as a tip, but the problem states that each friend paid ₹90. ₹90 + ₹20 = ₹110, which is the total amount the friends paid. Since they originally paid ₹300, the difference is ₹10, which is the tip.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puzzle = puzzles.puzzles('The_Missing_Dollar') # Based on user input pick a puzzle.\n",
    "\n",
    "print(puzzle)\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"hi I am Bob\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92164ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "response = chain.invoke([HumanMessage(content=\"hi i am bob\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c717cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am Bob', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={})]\n",
      "You're testing me! I think I do, though... You said your name is Bob, right?\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am Bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "021f566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "That's correct!\n",
      "\n",
      "Delhi is indeed the capital of India, and it is located in the continent of Asia. Specifically, it is situated in the north-central part of India, on the banks of the Yamuna River.\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Delhi is capital of India,It is located in Asia', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"That's correct!\\n\\nDelhi is indeed the capital of India, and it is located in the continent of Asia. Specifically, it is situated in the north-central part of India, on the banks of the Yamuna River.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='what is the capital of India', additional_kwargs={}, response_metadata={}), HumanMessage(content='where it is located', additional_kwargs={}, response_metadata={})]\n",
      "The capital of India, Delhi, is located in the continent of Asia. Specifically, it is situated in the north-central part of India, on the banks of the Yamuna River.\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Delhi is capital of India,It is located in Asia'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('what is the capital of India'))\n",
    "store.add_message(HumanMessage('where it is located'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "You just told me your name, Bob! I'm glad to know it. How's it going, Bob?\n"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9303",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Sandhya! Nice to meet you! Ahmednagar is a city in Maharashtra, India, right? What brings you here? Are you interested in chatting about something in particular or just saying hello? I'm all ears!\n",
      "You're from Ahmednagar, right? I've got it! Ahmednagar is a city in the state of Maharashtra, India. It's known for its rich history and cultural heritage. Is that correct?\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {\"session_id\": \"A1\"}}\n",
    "history = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = history.invoke([HumanMessage(content=\"hey!, I am SAndhya Ingale and I am from Ahmedanagar,\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = history.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know where i am from?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a798edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['abc2', 'A1'])\n"
     ]
    }
   ],
   "source": [
    "print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461c44f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
